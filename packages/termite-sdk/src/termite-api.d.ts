/**
 * This file was auto-generated by openapi-typescript.
 * Do not make direct changes to the file.
 */

export interface paths {
    "/embed": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Generate embeddings
         * @description Generates vector embeddings for input content using local ONNX models.
         *     This endpoint is compatible with Ollama's `/api/embed` format for text,
         *     and extends it with OpenAI-compatible multimodal support for CLIP models.
         *
         *     ## Models
         *
         *     Models are auto-discovered from `models_dir/embedders/` at startup.
         *     Use the `/api/models` endpoint to list available models.
         *
         *     - **Text-only models** (e.g., bge-small-en-v1.5): Accept text strings
         *     - **Multimodal models** (e.g., CLIP): Accept text and images via data URIs
         *
         *     ## Input Formats
         *
         *     Three formats are supported:
         *     - Single text string: `"hello world"`
         *     - Array of text strings: `["hello", "world"]` (Ollama-compatible)
         *     - Array of content parts: `[{"type": "text", "text": "..."}, {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}]` (OpenAI-compatible)
         *
         *     ## Caching
         *
         *     Results are cached in memory for 2 minutes. Concurrent identical requests are deduplicated
         *     using singleflight to prevent redundant work.
         *
         *     ## Response Formats
         *
         *     Supports multiple content types via Accept header:
         *     - `application/octet-stream`: Binary serialization (default, most efficient)
         *     - `application/json`: JSON response with model name and embeddings
         *
         *     ## Examples
         *
         *     Text embedding (Ollama-compatible):
         *     ```json
         *     {
         *       "model": "bge-small-en-v1.5",
         *       "input": ["hello world", "machine learning"]
         *     }
         *     ```
         *
         *     Multimodal embedding (OpenAI-compatible):
         *     ```json
         *     {
         *       "model": "clip-vit-base-patch32",
         *       "input": [
         *         {"type": "text", "text": "a photo of a cat"},
         *         {"type": "image_url", "image_url": {"url": "data:image/png;base64,iVBORw0..."}}
         *       ]
         *     }
         *     ```
         */
        post: operations["generateEmbeddings"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/chunk": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Chunk text into smaller segments
         * @description Splits text into smaller chunks using semantic or fixed-size chunking models.
         *
         *     ## Models
         *
         *     ### Fixed Chunking (always available)
         *     - Simple token-based splitting with overlap
         *     - Use model="fixed"
         *     - Fast and deterministic
         *
         *     ### ONNX Models
         *     - Semantic chunking based on content similarity
         *     - Models auto-discovered from `models_dir/chunkers/`
         *     - Falls back to fixed chunking if model fails
         *
         *     ## Caching
         *
         *     Results are cached in memory for 2 minutes. Cache key includes both config and text content.
         *
         *     ## Example
         *
         *     ```json
         *     {
         *       "text": "This is a long document...",
         *       "config": {
         *         "model": "fixed",
         *         "target_tokens": 500,
         *         "overlap_tokens": 50,
         *         "separator": "\n\n"
         *       }
         *     }
         *     ```
         */
        post: operations["chunkText"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/rerank": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Rerank prompts by relevance
         * @description Re-scores pre-rendered text prompts based on relevance to a query using ONNX reranking models.
         *
         *     ## Client Responsibilities
         *
         *     The client must:
         *     1. Extract relevant fields from documents
         *     2. Render any templates
         *     3. Send pre-rendered text strings as `prompts`
         *
         *     This design keeps Termite stateless and allows clients to customize rendering logic.
         *
         *     ## Models
         *
         *     - Models are auto-discovered from `models_dir/rerankers/`
         *     - Supports quantized models (`model_quantized.onnx`)
         *     - Automatically prefers quantized variants if available
         *
         *     ## Example
         *
         *     ```json
         *     {
         *       "model": "bge-reranker-v2-m3",
         *       "query": "machine learning applications",
         *       "prompts": [
         *         "Introduction to Machine Learning: This guide covers...",
         *         "Deep Learning Fundamentals: Neural networks are..."
         *       ]
         *     }
         *     ```
         *
         *     For document-based reranking with field extraction, use the client-side
         *     `lib/reranking` package which handles rendering before calling this endpoint.
         */
        post: operations["rerankPrompts"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/models": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * List available models
         * @description Returns lists of available embedding, chunking, and reranking models.
         *
         *     ## Embedders
         *
         *     - ONNX models from `models_dir/embedders/`
         *     - Quantized variants have `-i8` suffix
         *
         *     ## Chunkers
         *
         *     - Always includes "fixed" (built-in)
         *     - Plus any ONNX models from `models_dir/chunkers/`
         *
         *     ## Rerankers
         *
         *     - ONNX models from `models_dir/rerankers/`
         *     - Empty if no models configured
         *
         *     Models are discovered at service startup and cached.
         */
        get: operations["listModels"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/version": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * Get version information
         * @description Returns Termite version, git commit, build time, and Go runtime version.
         */
        get: operations["getVersion"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
}
export type webhooks = Record<string, never>;
export interface components {
    schemas: {
        Error: {
            /** @description Error message */
            error: string;
        };
        /** @description Text content for embedding */
        TextContentPart: {
            /** @enum {string} */
            type: "text";
            /** @description Text content to embed */
            text: string;
        };
        /** @description Image URL or data URI */
        ImageURL: {
            /**
             * @description URL or data URI (data:image/png;base64,...)
             * @example data:image/png;base64,iVBORw0KGgo...
             */
            url: string;
        };
        /** @description Image content for embedding (OpenAI-compatible format) */
        ImageURLContentPart: {
            /** @enum {string} */
            type: "image_url";
            image_url: components["schemas"]["ImageURL"];
        };
        /** @description A content part for multimodal embedding (text or image) */
        ContentPart: components["schemas"]["TextContentPart"] | components["schemas"]["ImageURLContentPart"];
        EmbedRequest: {
            /**
             * @description Name of the embedder model from models_dir/embedders/
             * @example bge-small-en-v1.5
             */
            model: string;
            /**
             * @description Input content to embed. Supports three formats:
             *     - Single text string: `"hello world"`
             *     - Array of text strings: `["hello", "world"]`
             *     - Array of content parts (multimodal): `[{"type": "text", "text": "hello"}, {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}]`
             * @example [
             *       "hello world",
             *       "machine learning"
             *     ]
             */
            input: string | string[] | components["schemas"]["ContentPart"][];
            /**
             * @description Truncate input to fit model context length
             * @default true
             */
            truncate?: boolean;
        };
        /**
         * @example {
         *       "model": "bge-small-en-v1.5",
         *       "embeddings": [
         *         [
         *           0.0123,
         *           -0.0456,
         *           0.0789
         *         ],
         *         [
         *           0.0234,
         *           -0.0567,
         *           0.089
         *         ]
         *       ]
         *     }
         */
        EmbedResponse: {
            /**
             * @description Model used for embedding
             * @example bge-small-en-v1.5
             */
            model: string;
            /** @description Array of embedding vectors (one per input string) */
            embeddings: number[][];
        };
        /** @description A chunk of text with position information. */
        Chunk: {
            /** @description Sequence number of the chunk (0, 1, 2, ...) */
            id: number;
            /** @description The chunk content */
            text: string;
            /** @description Character position in original text where chunk starts */
            start_char: number;
            /** @description Character position in original text where chunk ends (exclusive) */
            end_char: number;
        };
        /**
         * @description Configuration for chunking requests to Termite API.
         *     This is a simplified config for the HTTP API - differs from the full ChunkerConfig
         *     which includes provider selection and caching configuration.
         */
        ChunkConfig: {
            /**
             * @description The chunking model to use. Either 'fixed' for simple token-based chunking, or a model name from models/chunkers/{name}/.
             * @default fixed
             * @example fixed
             */
            model?: string;
            /**
             * @description Target number of tokens per chunk
             * @default 500
             * @example 500
             */
            target_tokens?: number;
            /**
             * @description Number of overlapping tokens between chunks
             * @default 50
             * @example 50
             */
            overlap_tokens?: number;
            /**
             * @description Text separator for fixed chunking
             * @default
             * @example
             */
            separator?: string;
            /**
             * @description Maximum number of chunks to return
             * @default 50
             * @example 50
             */
            max_chunks?: number;
            /**
             * Format: float
             * @description Confidence threshold for ONNX models (0.0-1.0)
             * @default 0.5
             * @example 0.5
             */
            threshold?: number;
        };
        ChunkRequest: {
            /**
             * @description Text to chunk
             * @example This is a long document that needs to be split into smaller chunks...
             */
            text: string;
            config?: components["schemas"]["ChunkConfig"];
        };
        /**
         * @example {
         *       "chunks": [
         *         {
         *           "id": 0,
         *           "text": "This is the first chunk...",
         *           "start_char": 0,
         *           "end_char": 100
         *         },
         *         {
         *           "id": 1,
         *           "text": "This is the second chunk...",
         *           "start_char": 90,
         *           "end_char": 190
         *         }
         *       ],
         *       "model": "fixed",
         *       "cache_hit": false
         *     }
         */
        ChunkResponse: {
            /** @description Array of text chunks */
            chunks: components["schemas"]["Chunk"][];
            /**
             * @description Chunking model actually used (may differ from requested if fallback occurred)
             * @example fixed
             */
            model: string;
            /** @description Whether result was served from cache */
            cache_hit: boolean;
        };
        RerankRequest: {
            /**
             * @description Name of reranking model from models_dir/rerankers/
             * @example bge-reranker-v2-m3
             */
            model: string;
            /**
             * @description Search query for relevance scoring
             * @example machine learning applications
             */
            query: string;
            /**
             * @description Pre-rendered document texts to rerank. The client is responsible for extracting
             *     and rendering document fields/templates before calling this endpoint.
             * @example [
             *       "Introduction to machine learning...",
             *       "Deep learning fundamentals..."
             *     ]
             */
            prompts: string[];
        };
        RerankResponse: {
            /** @description Name of model used for reranking */
            model: string;
            /** @description Relevance scores (one per prompt, same order as input) */
            scores: number[];
        };
        ModelsResponse: {
            /**
             * @description Available chunking models (always includes "fixed")
             * @example [
             *       "fixed",
             *       "chonky-mmbert-small-multilingual-1"
             *     ]
             */
            chunkers: string[];
            /**
             * @description Available reranking models
             * @example [
             *       "bge-reranker-v2-m3"
             *     ]
             */
            rerankers: string[];
            /**
             * @description Available embedding models from models_dir/embedders/
             * @example [
             *       "bge-small-en-v1.5",
             *       "bge-small-en-v1.5-i8-qt"
             *     ]
             */
            embedders: string[];
        };
        Config: {
            /**
             * Format: uri
             * @description URL of the Termite embedding/chunking service
             * @example http://localhost:8080
             */
            api_url: string;
            /**
             * @description Base directory containing model subdirectories. Termite auto-discovers models from:
             *     - `{models_dir}/embedders/` - Embedding models (ONNX)
             *     - `{models_dir}/chunkers/` - Chunking models (ONNX)
             *     - `{models_dir}/rerankers/` - Reranking models (ONNX)
             *
             *     Defaults to ~/.termite/models (set via viper). If not set, only built-in fixed chunking is available.
             * @example ~/.termite/models
             */
            models_dir?: string;
            /** @description Security settings for downloading content from URLs (e.g., images for CLIP models). Controls allowed hosts, private IP blocking, download limits, and timeouts. */
            content_security?: components["schemas"]["ContentSecurityConfig"];
            /** @description S3 credentials for downloading content from S3 URLs. If not set, S3 URLs will fail. */
            s3_credentials?: components["schemas"]["Credentials"];
            /**
             * @description How long to keep models loaded in memory after last use (Ollama-compatible).
             *     Models are automatically unloaded after this duration of inactivity.
             *     Use Go duration format: "5m" (5 minutes), "1h" (1 hour), "0" (never unload, eager loading).
             *     When set to "0" or omitted, models are loaded eagerly at startup and never unloaded (legacy behavior).
             * @default 0
             * @example 5m
             */
            keep_alive?: string;
            /**
             * @description Maximum number of models to keep loaded in memory simultaneously.
             *     When this limit is reached, the least recently used model is unloaded (LRU eviction).
             *     Set to 0 for unlimited (default). Only effective when keep_alive is non-zero.
             * @default 0
             * @example 3
             */
            max_loaded_models?: number;
            /**
             * @description Backend priority order for model loading with optional device specifiers.
             *     Format: `backend` or `backend:device` where device defaults to `auto`.
             *
             *     Termite tries entries in order and uses the first available backend+device
             *     combination that supports the model.
             *
             *     **Backends** (depend on build tags):
             *     - `go` - Pure Go inference (always available, CPU only, slowest)
             *     - `onnx` - ONNX Runtime (requires -tags="onnx,ORT", fastest)
             *     - `xla` - GoMLX XLA (requires -tags="xla,XLA", TPU/CUDA/CPU)
             *
             *     **Devices**:
             *     - `auto` - Auto-detect best available (default)
             *     - `cuda` - NVIDIA CUDA GPU
             *     - `coreml` - Apple CoreML (macOS only, used by ONNX)
             *     - `tpu` - Google TPU (used by XLA)
             *     - `cpu` - Force CPU only
             *
             *     **Examples**:
             *     - `["onnx", "xla", "go"]` - Try backends with auto device detection
             *     - `["onnx:cuda", "xla:tpu", "onnx:cpu", "go"]` - Prefer GPU, fall back to CPU
             *     - `["onnx:coreml", "go"]` - macOS with CoreML acceleration
             * @default [
             *       "onnx",
             *       "xla",
             *       "go"
             *     ]
             * @example [
             *       "onnx:cuda",
             *       "xla:tpu",
             *       "onnx:cpu",
             *       "xla:cpu",
             *       "go"
             *     ]
             */
            backend_priority?: string[];
            /**
             * @description Maximum number of concurrent inference requests allowed.
             *     Additional requests will be queued up to max_queue_size.
             *     Set to 0 for unlimited (default).
             * @default 0
             * @example 4
             */
            max_concurrent_requests?: number;
            /**
             * @description Maximum number of requests to queue when max_concurrent_requests is reached.
             *     When the queue is full, new requests receive 503 Service Unavailable with Retry-After header.
             *     Set to 0 for unlimited queue (default). Only effective when max_concurrent_requests > 0.
             * @default 0
             * @example 100
             */
            max_queue_size?: number;
            /**
             * @description Maximum time to wait for a request to complete, including queue wait time.
             *     Use Go duration format: "30s", "1m", "0" (no timeout, default).
             *     Requests exceeding this timeout receive 504 Gateway Timeout.
             * @default 0
             * @example 30s
             */
            request_timeout?: string;
            /**
             * @description List of model names to preload at startup (Ollama-compatible).
             *     These models are loaded immediately when Termite starts, avoiding first-request latency.
             *     Model names should match those in models_dir/embedders/ (e.g., "bge-small-en-v1.5").
             *     Only effective when keep_alive is non-zero (lazy loading mode).
             * @example [
             *       "bge-small-en-v1.5",
             *       "clip-vit-base-patch32"
             *     ]
             */
            preload?: string[];
            /**
             * @description Maximum memory (in MB) to use for loaded models.
             *     When this limit is approached, least recently used models are unloaded.
             *     Set to 0 for unlimited (default). This is an advisory limit - actual memory
             *     usage depends on model sizes and may temporarily exceed this value.
             *     Works alongside max_loaded_models for fine-grained control.
             * @default 0
             * @example 4096
             */
            max_memory_mb?: number;
            /**
             * @description Per-model loading strategy overrides. Maps model names to their loading strategy.
             *     Models not in this map use the default strategy based on keep_alive:
             *     - If keep_alive="0" (default): eager loading (load at startup, never unload)
             *     - If keep_alive>0: lazy loading (load on demand, unload after idle)
             *
             *     When a model has strategy "eager" in this map:
             *     - It is loaded at startup (as part of preload)
             *     - It is never unloaded, even when keep_alive>0 (pinned in memory)
             *
             *     This allows mixing eager and lazy models in the same pool.
             * @example {
             *       "bge-small-en-v1.5": "eager",
             *       "chonky": "lazy"
             *     }
             */
            model_strategies?: {
                [key: string]: "eager" | "lazy" | "bounded";
            };
            log?: components["schemas"]["schemas-Config"];
        };
        VersionResponse: {
            /**
             * @description Termite version
             * @example v1.0.0
             */
            version: string;
            /**
             * @description Git commit hash
             * @example abc1234
             */
            git_commit: string;
            /**
             * @description Build timestamp
             * @example 2024-01-15T10:30:00Z
             */
            build_time: string;
            /**
             * @description Go runtime version
             * @example go1.25.0
             */
            go_version: string;
        };
        ContentSecurityConfig: {
            /**
             * @description Whitelist of allowed hostnames/IPs for link downloads. If empty, all hosts are allowed (except private IPs if block_private_ips is true).
             * @example [
             *       "example.com",
             *       "cdn.example.com",
             *       "192.0.2.1"
             *     ]
             */
            allowed_hosts?: string[];
            /**
             * @description Block requests to private IP ranges (127.0.0.0/8, 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16, 169.254.0.0/16)
             * @default true
             */
            block_private_ips?: boolean;
            /**
             * @description Maximum size of downloaded content in bytes
             * @default 104857600
             * @example 104857600
             */
            max_download_size_bytes?: number;
            /**
             * @description Timeout for individual download operations in seconds
             * @default 30
             * @example 30
             */
            download_timeout_seconds?: number;
            /**
             * @description Maximum image width/height in pixels (images will be resized)
             * @default 2048
             * @example 2048
             */
            max_image_dimension?: number;
            /**
             * @description Whitelist of allowed path prefixes for file:// and s3:// URLs. If empty, all paths are allowed. For file:// use absolute paths (e.g., /Users/data/). For s3:// use bucket/prefix (e.g., my-bucket/uploads/).
             * @example [
             *       "/Users/data/",
             *       "my-bucket/uploads/"
             *     ]
             */
            allowed_paths?: string[];
        };
        Credentials: {
            /**
             * @description S3-compatible endpoint (e.g., 's3.amazonaws.com' or 'localhost:9000' for MinIO)
             * @example s3.amazonaws.com
             */
            endpoint?: string;
            /**
             * @description Enable SSL/TLS for S3 connections (default: true for AWS, false for local MinIO)
             * @default true
             */
            use_ssl?: boolean;
            /**
             * @description AWS access key ID. Supports keystore syntax for secret lookup. Falls back to AWS_ACCESS_KEY_ID environment variable if not set.
             * @example your-access-key-id
             */
            access_key_id?: string;
            /**
             * @description AWS secret access key. Supports keystore syntax for secret lookup. Falls back to AWS_SECRET_ACCESS_KEY environment variable if not set.
             * @example your-secret-access-key
             */
            secret_access_key?: string;
            /**
             * @description Optional AWS session token for temporary credentials. Supports keystore syntax for secret lookup.
             * @example your-session-token
             */
            session_token?: string;
        };
        /**
         * @description Logging verbosity level
         * @default info
         * @example info
         * @enum {string}
         */
        Level: "debug" | "info" | "warn" | "error";
        /**
         * @description Logging output format style. 'terminal' for colorized console, 'json' for structured JSON, 'logfmt' for token-efficient key=value pairs, 'noop' for silent.
         * @default terminal
         * @example terminal
         * @enum {string}
         */
        Style: "terminal" | "json" | "logfmt" | "noop";
        /** @description Logging configuration for Termite services */
        "schemas-Config": {
            level?: components["schemas"]["Level"];
            style?: components["schemas"]["Style"];
        };
    };
    responses: never;
    parameters: never;
    requestBodies: never;
    headers: never;
    pathItems: never;
}
export type $defs = Record<string, never>;
export interface operations {
    generateEmbeddings: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["EmbedRequest"];
            };
        };
        responses: {
            /** @description Embeddings generated successfully */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/octet-stream": string;
                    "application/json": components["schemas"]["EmbedResponse"];
                };
            };
            /** @description Invalid request */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Model not found */
            404: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Internal server error */
            500: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
        };
    };
    chunkText: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["ChunkRequest"];
            };
        };
        responses: {
            /** @description Text chunked successfully */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["ChunkResponse"];
                };
            };
            /** @description Invalid request */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Internal server error */
            500: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
        };
    };
    rerankPrompts: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["RerankRequest"];
            };
        };
        responses: {
            /** @description Prompts reranked successfully */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["RerankResponse"];
                };
            };
            /** @description Invalid request */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Model not found */
            404: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Internal server error */
            500: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Reranking service unavailable (no models configured) */
            503: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
        };
    };
    listModels: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Models retrieved successfully */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["ModelsResponse"];
                };
            };
            /** @description Bad request */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Internal server error */
            500: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
        };
    };
    getVersion: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Version information */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["VersionResponse"];
                };
            };
            /** @description Bad request */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
        };
    };
}

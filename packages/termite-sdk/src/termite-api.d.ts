/**
 * This file was auto-generated by openapi-typescript.
 * Do not make direct changes to the file.
 */

export interface paths {
    "/embed": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Generate embeddings
         * @description Generates vector embeddings for input content using local ONNX models.
         *     This endpoint is compatible with Ollama's `/api/embed` format for text,
         *     and extends it with OpenAI-compatible multimodal support for CLIP models.
         *
         *     ## Models
         *
         *     Models are auto-discovered from `models_dir/embedders/` at startup.
         *     Use the `/api/models` endpoint to list available models.
         *
         *     - **Text-only models** (e.g., BAAI/bge-small-en-v1.5): Accept text strings
         *     - **Multimodal models** (e.g., CLIP): Accept text and images via data URIs
         *
         *     ## Input Formats
         *
         *     Three formats are supported:
         *     - Single text string: `"hello world"`
         *     - Array of text strings: `["hello", "world"]` (Ollama-compatible)
         *     - Array of content parts: `[{"type": "text", "text": "..."}, {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}]` (OpenAI-compatible)
         *
         *     ## Caching
         *
         *     Results are cached in memory for 2 minutes. Concurrent identical requests are deduplicated
         *     using singleflight to prevent redundant work.
         *
         *     ## Response Formats
         *
         *     Supports multiple content types via Accept header:
         *     - `application/octet-stream`: Binary serialization (default, most efficient)
         *     - `application/json`: JSON response with model name and embeddings
         *
         *     ## Examples
         *
         *     Text embedding (Ollama-compatible):
         *     ```json
         *     {
         *       "model": "BAAI/bge-small-en-v1.5",
         *       "input": ["hello world", "machine learning"]
         *     }
         *     ```
         *
         *     Multimodal embedding (OpenAI-compatible):
         *     ```json
         *     {
         *       "model": "openai/clip-vit-base-patch32",
         *       "input": [
         *         {"type": "text", "text": "a photo of a cat"},
         *         {"type": "image_url", "image_url": {"url": "data:image/png;base64,iVBORw0..."}}
         *       ]
         *     }
         *     ```
         */
        post: operations["generateEmbeddings"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/chunk": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Chunk text into smaller segments
         * @description Splits text into smaller chunks using semantic or fixed-size chunking models.
         *
         *     ## Models
         *
         *     ### Fixed Chunking (always available)
         *     - Simple token-based splitting with overlap
         *     - Use model="fixed"
         *     - Fast and deterministic
         *
         *     ### ONNX Models
         *     - Semantic chunking based on content similarity
         *     - Models auto-discovered from `models_dir/chunkers/`
         *     - Falls back to fixed chunking if model fails
         *
         *     ## Caching
         *
         *     Results are cached in memory for 2 minutes. Cache key includes both config and text content.
         *
         *     ## Example
         *
         *     ```json
         *     {
         *       "text": "This is a long document...",
         *       "config": {
         *         "model": "fixed",
         *         "target_tokens": 500,
         *         "overlap_tokens": 50,
         *         "separator": "\n\n"
         *       }
         *     }
         *     ```
         */
        post: operations["chunkText"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/rerank": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Rerank prompts by relevance
         * @description Re-scores pre-rendered text prompts based on relevance to a query using ONNX reranking models.
         *
         *     ## Client Responsibilities
         *
         *     The client must:
         *     1. Extract relevant fields from documents
         *     2. Render any templates
         *     3. Send pre-rendered text strings as `prompts`
         *
         *     This design keeps Termite stateless and allows clients to customize rendering logic.
         *
         *     ## Models
         *
         *     - Models are auto-discovered from `models_dir/rerankers/`
         *     - Supports quantized models (`model_quantized.onnx`)
         *     - Automatically prefers quantized variants if available
         *
         *     ## Example
         *
         *     ```json
         *     {
         *       "model": "BAAI/bge-reranker-v2-m3",
         *       "query": "machine learning applications",
         *       "prompts": [
         *         "Introduction to Machine Learning: This guide covers...",
         *         "Deep Learning Fundamentals: Neural networks are..."
         *       ]
         *     }
         *     ```
         *
         *     For document-based reranking with field extraction, use the client-side
         *     `lib/reranking` package which handles rendering before calling this endpoint.
         */
        post: operations["rerankPrompts"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/generate": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Generate text using LLM (OpenAI-compatible)
         * @description Generates text using local LLM models (e.g., Gemma 3).
         *     Fully compatible with the OpenAI Chat Completions API.
         *
         *     ## Models
         *
         *     Models are auto-discovered from `models_dir/generators/` at startup.
         *     Use the `/api/models` endpoint to list available models.
         *
         *     ## Streaming
         *
         *     Set `stream: true` to receive Server-Sent Events (SSE) with incremental
         *     token deltas. Each event contains a `ChatCompletionChunk` object.
         *     The stream ends with `data: [DONE]`.
         *
         *     ## Input Format
         *
         *     Uses OpenAI-compatible chat format with messages array:
         *     ```json
         *     {
         *       "model": "google/gemma-3-1b-it",
         *       "messages": [
         *         {"role": "system", "content": "You are a helpful assistant."},
         *         {"role": "user", "content": "Hello!"}
         *       ],
         *       "max_tokens": 256,
         *       "stream": false
         *     }
         *     ```
         *
         *     ## Example (Non-streaming)
         *
         *     ```bash
         *     curl -X POST http://localhost:8080/api/generate \
         *       -H "Content-Type: application/json" \
         *       -d '{
         *         "model": "google/gemma-3-1b-it",
         *         "messages": [{"role": "user", "content": "What is machine learning?"}],
         *         "max_tokens": 100
         *       }'
         *     ```
         *
         *     ## Example (Streaming)
         *
         *     ```bash
         *     curl -X POST http://localhost:8080/api/generate \
         *       -H "Content-Type: application/json" \
         *       -d '{
         *         "model": "google/gemma-3-1b-it",
         *         "messages": [{"role": "user", "content": "Hello!"}],
         *         "stream": true
         *       }'
         *     ```
         */
        post: operations["generateContent"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/recognize": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Recognize named entities
         * @description Recognizes named entities (persons, organizations, locations, etc.) from text using ONNX recognition models.
         *
         *     ## Entity Types
         *
         *     Standard CoNLL entity types:
         *     - **PER**: Person names (e.g., "John Smith")
         *     - **ORG**: Organizations (e.g., "Google", "Apple Inc.")
         *     - **LOC**: Locations (e.g., "New York", "France")
         *     - **MISC**: Miscellaneous entities
         *
         *     ## Models
         *
         *     - Models are auto-discovered from `models_dir/recognizers/`
         *     - Supports quantized variants (model_i8.onnx)
         *     - Compatible with HuggingFace BERT-based recognition models
         *     - GLiNER models support custom entity labels via the `labels` parameter
         *
         *     ## Example
         *
         *     ```json
         *     {
         *       "model": "dslim/bert-base-NER",
         *       "texts": ["John Smith works at Google.", "Apple Inc. is in Cupertino."]
         *     }
         *     ```
         */
        post: operations["recognizeEntities"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/classify": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Zero-shot text classification
         * @description Classifies text into arbitrary categories using NLI-based zero-shot classification models.
         *
         *     ## How It Works
         *
         *     Zero-shot classification uses Natural Language Inference (NLI) to classify text
         *     without requiring training data for the specific categories. The model determines
         *     how well a text "entails" each candidate label.
         *
         *     ## Models
         *
         *     - Models are auto-discovered from `models_dir/classifiers/`
         *     - Supports multilingual models like mDeBERTa-mnli-xnli
         *     - Compatible with HuggingFace NLI/MNLI models exported to ONNX
         *
         *     ## Use Cases
         *
         *     - **Sentiment Analysis**: Classify as positive/negative/neutral
         *     - **Topic Classification**: Categorize by topic without training
         *     - **Intent Detection**: Identify user intents from text
         *     - **Content Moderation**: Detect inappropriate content types
         *
         *     ## Example
         *
         *     ```json
         *     {
         *       "model": "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli",
         *       "texts": ["I love this product!", "The service was terrible."],
         *       "labels": ["positive", "negative", "neutral"]
         *     }
         *     ```
         *
         *     ## Multilingual Support
         *
         *     The mDeBERTa-mnli-xnli model supports 100+ languages. You can classify text
         *     in any supported language using labels in that language:
         *
         *     ```json
         *     {
         *       "model": "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli",
         *       "texts": ["J'adore ce produit!"],
         *       "labels": ["positif", "n√©gatif", "neutre"]
         *     }
         *     ```
         */
        post: operations["classifyText"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/rewrite": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Rewrite text using Seq2Seq models
         * @description Rewrite/transform text using Seq2Seq models (T5, FLAN-T5, BART, etc.).
         *
         *     ## Models
         *
         *     - Models are auto-discovered from `models_dir/rewriters/`
         *     - Seq2Seq models have encoder.onnx, decoder-init.onnx, and decoder.onnx files
         *     - Compatible with LMQG question generation models
         *
         *     ## Use Cases
         *
         *     - **Question Generation**: Generate questions from answer-context pairs
         *     - **Query Generation**: Generate search queries from documents
         *     - **Paraphrasing**: Rewrite text in different words
         *     - **Translation**: Translate text between languages
         *
         *     ## Example
         *
         *     For question generation with LMQG models:
         *     ```json
         *     {
         *       "model": "lmqg/flan-t5-small-squad-qg",
         *       "inputs": ["generate question: <hl> Beyonce <hl> Beyonce starred as Etta James in Cadillac Records."]
         *     }
         *     ```
         */
        post: operations["rewriteText"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/read": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Read text from images (OCR/document understanding)
         * @description Extracts text from images using Vision2Seq models like TrOCR, Donut, or Florence-2.
         *
         *     ## Models
         *
         *     Models are auto-discovered from `models_dir/readers/` at startup.
         *     Use the `/api/models` endpoint to list available models.
         *
         *     - **TrOCR**: Pure OCR for printed/handwritten text
         *     - **Donut**: Document understanding with structured output (receipts, forms)
         *     - **Florence-2**: Multi-task vision model (OCR, captioning, VQA)
         *
         *     ## Task Prompts
         *
         *     Some models support task prompts for different extraction modes:
         *
         *     - **Donut CORD**: `<s_cord-v2>` for receipt parsing
         *     - **Donut DocVQA**: `<s_docvqa><s_question>...</s_question><s_answer>` for visual QA
         *     - **Florence-2 OCR**: `<OCR>` for text extraction
         *     - **Florence-2 Caption**: `<CAPTION>` for image description
         *
         *     ## Example
         *
         *     ```json
         *     {
         *       "model": "microsoft/trocr-base-printed",
         *       "images": [
         *         {"url": "data:image/png;base64,iVBORw0KGgo..."}
         *       ],
         *       "max_tokens": 256
         *     }
         *     ```
         *
         *     With Donut for receipt parsing:
         *     ```json
         *     {
         *       "model": "naver-clova-ix/donut-base-finetuned-cord-v2",
         *       "images": [{"url": "data:image/png;base64,..."}],
         *       "prompt": "<s_cord-v2>"
         *     }
         *     ```
         */
        post: operations["readImages"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/transcribe": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Transcribe audio to text (speech-to-text)
         * @description Transcribes audio to text using Speech2Seq models like Whisper, Wav2Vec2, or HuBERT.
         *
         *     ## Models
         *
         *     Models are auto-discovered from `models_dir/transcribers/` at startup.
         *     Use the `/api/models` endpoint to list available models.
         *
         *     - **Whisper**: OpenAI's Whisper models (multilingual, automatic language detection)
         *     - **Wav2Vec2**: Facebook's Wav2Vec 2.0 models (English-focused)
         *     - **HuBERT**: Facebook's HuBERT models (self-supervised)
         *
         *     ## Audio Input
         *
         *     Audio data should be base64-encoded. Supported formats depend on the model:
         *     - WAV (recommended - raw PCM)
         *     - MP3
         *     - FLAC
         *     - M4A/AAC
         *
         *     ## Example
         *
         *     ```json
         *     {
         *       "model": "openai/whisper-tiny",
         *       "audio": "UklGRi..."
         *     }
         *     ```
         *
         *     With language hint:
         *     ```json
         *     {
         *       "model": "openai/whisper-tiny",
         *       "audio": "UklGRi...",
         *       "language": "en"
         *     }
         *     ```
         */
        post: operations["transcribeAudio"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/models": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * List available models
         * @description Returns lists of available embedding, chunking, reranking, generator, NER, rewriter, reader, and transcriber models.
         *
         *     ## Embedders
         *
         *     - ONNX models from `models_dir/embedders/`
         *     - Quantized variants have `-i8` suffix
         *
         *     ## Chunkers
         *
         *     - Always includes "fixed" (built-in)
         *     - Plus any ONNX models from `models_dir/chunkers/`
         *
         *     ## Rerankers
         *
         *     - ONNX models from `models_dir/rerankers/`
         *     - Empty if no models configured
         *
         *     ## Generators
         *
         *     - LLM models from `models_dir/generators/`
         *     - Empty if no models configured
         *
         *     ## Recognizers
         *
         *     - ONNX models from `models_dir/recognizers/`
         *     - Includes GLiNER models for zero-shot recognition
         *
         *     ## Rewriters
         *
         *     - Seq2Seq models from `models_dir/rewriters/`
         *     - T5, FLAN-T5, BART, and LMQG question generation models
         *
         *     ## Readers
         *
         *     - Vision2Seq models from `models_dir/readers/`
         *     - TrOCR, Donut, Florence-2 for OCR and document understanding
         *
         *     ## Transcribers
         *
         *     - Speech2Seq models from `models_dir/transcribers/`
         *     - Whisper, Wav2Vec2, HuBERT for speech-to-text
         *
         *     Models are discovered at service startup and cached.
         */
        get: operations["listModels"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/version": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * Get version information
         * @description Returns Termite version, git commit, build time, and Go runtime version.
         */
        get: operations["getVersion"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
}
export type webhooks = Record<string, never>;
export interface components {
    schemas: {
        Error: {
            /** @description Error message */
            error: string;
        };
        /** @description Text content for embedding */
        TextContentPart: {
            /** @enum {string} */
            type: "text";
            /** @description Text content to embed */
            text: string;
        };
        /** @description Image URL or data URI */
        ImageURL: {
            /**
             * @description URL or data URI (data:image/png;base64,...)
             * @example data:image/png;base64,iVBORw0KGgo...
             */
            url: string;
        };
        /** @description Image content for embedding (OpenAI-compatible format) */
        ImageURLContentPart: {
            /** @enum {string} */
            type: "image_url";
            image_url: components["schemas"]["ImageURL"];
        };
        /** @description A content part for multimodal embedding (text or image) */
        ContentPart: components["schemas"]["TextContentPart"] | components["schemas"]["ImageURLContentPart"];
        EmbedRequest: {
            /**
             * @description Name of the embedder model from models_dir/embedders/
             * @example BAAI/bge-small-en-v1.5
             */
            model: string;
            /**
             * @description Input content to embed. Supports three formats:
             *     - Single text string: `"hello world"`
             *     - Array of text strings: `["hello", "world"]`
             *     - Array of content parts (multimodal): `[{"type": "text", "text": "hello"}, {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}]`
             * @example [
             *       "hello world",
             *       "machine learning"
             *     ]
             */
            input: string | string[] | components["schemas"]["ContentPart"][];
            /**
             * @description Truncate input to fit model context length
             * @default true
             */
            truncate?: boolean;
        };
        /**
         * @example {
         *       "model": "BAAI/bge-small-en-v1.5",
         *       "embeddings": [
         *         [
         *           0.0123,
         *           -0.0456,
         *           0.0789
         *         ],
         *         [
         *           0.0234,
         *           -0.0567,
         *           0.089
         *         ]
         *       ]
         *     }
         */
        EmbedResponse: {
            /**
             * @description Model used for embedding
             * @example BAAI/bge-small-en-v1.5
             */
            model: string;
            /** @description Array of embedding vectors (one per input string) */
            embeddings: number[][];
        };
        /** @description A chunk of text with position information. */
        Chunk: {
            /** @description Sequence number of the chunk (0, 1, 2, ...) */
            id: number;
            /** @description The chunk content */
            text: string;
            /** @description Character position in original text where chunk starts */
            start_char: number;
            /** @description Character position in original text where chunk ends (exclusive) */
            end_char: number;
        };
        /**
         * @description Configuration for chunking requests to Termite API.
         *     This is a simplified config for the HTTP API - differs from the full ChunkerConfig
         *     which includes provider selection and caching configuration.
         */
        ChunkConfig: {
            /**
             * @description The chunking model to use. Either 'fixed' for simple token-based chunking, or a model name from models/chunkers/{name}/.
             * @default fixed
             * @example fixed
             */
            model?: string;
            /**
             * @description Target number of tokens per chunk
             * @default 500
             * @example 500
             */
            target_tokens?: number;
            /**
             * @description Number of overlapping tokens between chunks
             * @default 50
             * @example 50
             */
            overlap_tokens?: number;
            /**
             * @description Text separator for fixed chunking
             * @default
             * @example
             */
            separator?: string;
            /**
             * @description Maximum number of chunks to return
             * @default 50
             * @example 50
             */
            max_chunks?: number;
            /**
             * Format: float
             * @description Confidence threshold for ONNX models (0.0-1.0)
             * @default 0.5
             * @example 0.5
             */
            threshold?: number;
        };
        ChunkRequest: {
            /**
             * @description Text to chunk
             * @example This is a long document that needs to be split into smaller chunks...
             */
            text: string;
            config?: components["schemas"]["ChunkConfig"];
        };
        /**
         * @example {
         *       "chunks": [
         *         {
         *           "id": 0,
         *           "text": "This is the first chunk...",
         *           "start_char": 0,
         *           "end_char": 100
         *         },
         *         {
         *           "id": 1,
         *           "text": "This is the second chunk...",
         *           "start_char": 90,
         *           "end_char": 190
         *         }
         *       ],
         *       "model": "fixed",
         *       "cache_hit": false
         *     }
         */
        ChunkResponse: {
            /** @description Array of text chunks */
            chunks: components["schemas"]["Chunk"][];
            /**
             * @description Chunking model actually used (may differ from requested if fallback occurred)
             * @example fixed
             */
            model: string;
            /** @description Whether result was served from cache */
            cache_hit: boolean;
        };
        RerankRequest: {
            /**
             * @description Name of reranking model from models_dir/rerankers/
             * @example BAAI/bge-reranker-v2-m3
             */
            model: string;
            /**
             * @description Search query for relevance scoring
             * @example machine learning applications
             */
            query: string;
            /**
             * @description Pre-rendered document texts to rerank. The client is responsible for extracting
             *     and rendering document fields/templates before calling this endpoint.
             * @example [
             *       "Introduction to machine learning...",
             *       "Deep learning fundamentals..."
             *     ]
             */
            prompts: string[];
        };
        RerankResponse: {
            /** @description Name of model used for reranking */
            model: string;
            /** @description Relevance scores (one per prompt, same order as input) */
            scores: number[];
        };
        RecognizeEntity: {
            /**
             * @description The entity text
             * @example John Smith
             */
            text: string;
            /**
             * @description Entity type (PER, ORG, LOC, MISC)
             * @example PER
             */
            label: string;
            /**
             * @description Character offset where entity begins
             * @example 0
             */
            start: number;
            /**
             * @description Character offset where entity ends (exclusive)
             * @example 10
             */
            end: number;
            /**
             * Format: float
             * @description Confidence score (0.0 to 1.0)
             * @example 0.99
             */
            score: number;
        };
        RecognizeRequest: {
            /**
             * @description Name of recognizer model from models_dir/recognizers/
             * @example dslim/bert-base-NER
             */
            model: string;
            /**
             * @description Texts to extract entities from
             * @example [
             *       "John Smith works at Google.",
             *       "Apple Inc. is in Cupertino."
             *     ]
             */
            texts: string[];
            /**
             * @description Custom entity labels to extract (GLiNER models only).
             *     When using a GLiNER model, you can specify any entity types to extract,
             *     enabling zero-shot NER without model retraining.
             *     If not provided, the model's default labels are used.
             * @example [
             *       "person",
             *       "company",
             *       "product",
             *       "date"
             *     ]
             */
            labels?: string[];
            /**
             * @description Relation types to extract (for models with 'relations' capability).
             *     Only used when the model supports relation extraction (GLiNER multitask, REBEL).
             *     If not provided, the model extracts all relations it can detect.
             * @example [
             *       "founded",
             *       "works_at",
             *       "located_in"
             *     ]
             */
            relation_labels?: string[];
        };
        Relation: {
            /** @description The subject/head entity in the relationship */
            head: components["schemas"]["RecognizeEntity"];
            /** @description The object/tail entity in the relationship */
            tail: components["schemas"]["RecognizeEntity"];
            /**
             * @description The relationship type
             * @example founded
             */
            label: string;
            /**
             * Format: float
             * @description Confidence score for the relation (0.0 to 1.0)
             * @example 0.95
             */
            score: number;
        };
        RecognizeResponse: {
            /** @description Name of model used for NER */
            model: string;
            /** @description Array of entity arrays (one per input text) */
            entities: components["schemas"]["RecognizeEntity"][][];
            /**
             * @description Array of relation arrays (one per input text).
             *     Only present when using a model with 'relations' capability (GLiNER multitask, REBEL).
             */
            relations?: components["schemas"]["Relation"][][];
        };
        RewriteRequest: {
            /**
             * @description Name of Seq2Seq rewriter model from models_dir/rewriters/
             * @example lmqg/flan-t5-small-squad-qg
             */
            model: string;
            /**
             * @description Input texts to rewrite/transform
             * @example [
             *       "Translate to German: Hello, how are you?"
             *     ]
             */
            inputs: string[];
        };
        RewriteResponse: {
            /** @description Name of model used for rewriting */
            model: string;
            /** @description Rewritten texts (array of arrays, one per input, multiple per beam) */
            texts: string[][];
        };
        ClassifyRequest: {
            /**
             * @description Name of classifier model from models_dir/classifiers/
             * @example MoritzLaurer/mDeBERTa-v3-base-mnli-xnli
             */
            model: string;
            /**
             * @description Texts to classify
             * @example [
             *       "I love this product!",
             *       "The service was terrible."
             *     ]
             */
            texts: string[];
            /**
             * @description Candidate labels for zero-shot classification.
             *     The model will predict which label(s) best describe each text.
             * @example [
             *       "positive",
             *       "negative",
             *       "neutral"
             *     ]
             */
            labels: string[];
            /**
             * @description Custom hypothesis template for NLI-based classification.
             *     Use "{}" as placeholder for the label.
             *     Default: "This example is {}."
             * @example This text expresses a {} sentiment.
             */
            hypothesis_template?: string;
            /**
             * @description If true, allows multiple labels per text (independent scoring).
             *     If false (default), scores are normalized across labels.
             * @default false
             */
            multi_label?: boolean;
        };
        ClassifyResult: {
            /**
             * @description The predicted class/category
             * @example positive
             */
            label: string;
            /**
             * Format: float
             * @description Confidence score (0.0 to 1.0)
             * @example 0.95
             */
            score: number;
        };
        ClassifyResponse: {
            /** @description Name of model used for classification */
            model: string;
            /**
             * @description Array of classification results (one per input text).
             *     Each result is an array of ClassifyResult sorted by score descending.
             */
            classifications: components["schemas"]["ClassifyResult"][][];
        };
        ReadRequest: {
            /**
             * @description Name of reader model from models_dir/readers/
             * @example microsoft/trocr-base-printed
             */
            model: string;
            /**
             * @description Images to read text from. Supports:
             *     - Data URIs: `data:image/png;base64,...`
             *     - URLs (if content_security allows)
             * @example [
             *       {
             *         "url": "data:image/png;base64,iVBORw0KGgo..."
             *       }
             *     ]
             */
            images: components["schemas"]["ImageURL"][];
            /**
             * @description Optional task prompt for document understanding models.
             *     - TrOCR: Not used (pure OCR)
             *     - Donut CORD: "<s_cord-v2>" for receipt parsing
             *     - Donut DocVQA: "<s_docvqa><s_question>What is the total?</s_question><s_answer>"
             *     - Florence-2: "<OCR>" for OCR, "<CAPTION>" for captioning
             * @example <s_cord-v2>
             */
            prompt?: string;
            /**
             * @description Maximum tokens to generate
             * @default 256
             * @example 256
             */
            max_tokens?: number;
        };
        ReadResult: {
            /**
             * @description Extracted text from the image
             * @example Invoice Total: $123.45
             */
            text: string;
            /**
             * @description Structured fields extracted by document understanding models (Donut, Florence-2).
             *     Fields are flattened with dot notation for nested structures.
             *     Only present for models that output structured data.
             * @example {
             *       "menu.nm": "Coffee",
             *       "menu.price": "$3.50",
             *       "total": "$123.45"
             *     }
             */
            fields?: {
                [key: string]: string;
            };
        };
        ReadResponse: {
            /** @description Name of model used for reading */
            model: string;
            /** @description Array of read results (one per input image) */
            results: components["schemas"]["ReadResult"][];
        };
        TranscribeRequest: {
            /**
             * @description Name of transcriber model from models_dir/transcribers/
             * @example openai/whisper-tiny
             */
            model?: string;
            /**
             * Format: byte
             * @description Base64-encoded audio data (WAV, MP3, FLAC, etc.)
             */
            audio: string;
            /**
             * @description Force specific language for transcription (optional, model-dependent)
             * @example en
             */
            language?: string;
        };
        TranscribeResponse: {
            /** @description Name of model used for transcription */
            model: string;
            /**
             * @description Transcribed text from the audio
             * @example Hello, how are you today?
             */
            text: string;
            /**
             * @description Detected or forced language
             * @example en
             */
            language?: string;
        };
        ModelsResponse: {
            /**
             * @description Available chunking models (always includes "fixed")
             * @example [
             *       "fixed",
             *       "mirth/chonky-mmbert-small-multilingual-1"
             *     ]
             */
            chunkers: string[];
            /**
             * @description Available reranking models
             * @example [
             *       "BAAI/bge-reranker-v2-m3"
             *     ]
             */
            rerankers: string[];
            /**
             * @description Available zero-shot classification models
             * @example [
             *       "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli"
             *     ]
             */
            classifiers: string[];
            /**
             * @description Available embedding models from models_dir/embedders/
             * @example [
             *       "BAAI/bge-small-en-v1.5",
             *       "BAAI/bge-small-en-v1.5:i8"
             *     ]
             */
            embedders: string[];
            /**
             * @description Available generator/LLM models from models_dir/generators/
             * @example [
             *       "google/gemma-3-1b-it",
             *       "onnxruntime/Gemma-3-ONNX"
             *     ]
             */
            generators: string[];
            /**
             * @description Available recognizer models from models_dir/recognizers/
             * @example [
             *       "dslim/bert-base-NER",
             *       "dslim/bert-large-NER",
             *       "onnx-community/gliner_small-v2.1"
             *     ]
             */
            recognizers: string[];
            /**
             * @description Available GLiNER extractor models (zero-shot recognition with custom labels)
             * @example [
             *       "onnx-community/gliner_small-v2.1",
             *       "onnx-community/gliner-multitask"
             *     ]
             */
            extractors?: string[];
            /**
             * @description Available Seq2Seq rewriter models from models_dir/rewriters/
             * @example [
             *       "lmqg/flan-t5-small-squad-qg",
             *       "lmqg/flan-t5-base-squad-qg"
             *     ]
             */
            rewriters: string[];
            /**
             * @description Available reader/OCR models from models_dir/readers/
             * @example [
             *       "microsoft/trocr-base-printed",
             *       "naver-clova-ix/donut-base-finetuned-cord-v2"
             *     ]
             */
            readers: string[];
            /**
             * @description Available transcriber/speech-to-text models from models_dir/transcribers/
             * @example [
             *       "openai/whisper-tiny",
             *       "openai/whisper-base"
             *     ]
             */
            transcribers: string[];
            /**
             * @description Detailed information about recognizer models including capabilities.
             *     Map of model name to model info. Use this to determine what capabilities
             *     each recognizer supports (labels, zeroshot, relations, answers).
             * @example {
             *       "dslim/bert-base-NER": {
             *         "capabilities": [
             *           "labels"
             *         ]
             *       },
             *       "onnx-community/gliner_small-v2.1": {
             *         "capabilities": [
             *           "labels",
             *           "zeroshot"
             *         ]
             *       },
             *       "onnx-community/gliner-multitask": {
             *         "capabilities": [
             *           "labels",
             *           "zeroshot",
             *           "relations",
             *           "answers"
             *         ]
             *       }
             *     }
             */
            recognizer_info?: {
                [key: string]: components["schemas"]["RecognizerModelInfo"];
            };
        };
        /**
         * @description Capability that a recognizer model supports:
         *     - labels: Entity extraction (NER) - extracts labeled spans like PER, ORG, LOC
         *     - zeroshot: Supports arbitrary labels at inference time (GLiNER models)
         *     - relations: Relation extraction between entities (GLiNER multitask, REBEL)
         *     - answers: Extractive question answering (GLiNER multitask)
         * @enum {string}
         */
        RecognizerCapability: "labels" | "zeroshot" | "relations" | "answers";
        /** @description Detailed information about a recognizer model */
        RecognizerModelInfo: {
            /**
             * @description List of capabilities this recognizer model supports
             * @example [
             *       "labels",
             *       "zeroshot"
             *     ]
             */
            capabilities?: components["schemas"]["RecognizerCapability"][];
        };
        /** @description A tool (function) that the model can call */
        Tool: {
            /**
             * @description The type of tool (currently only "function" is supported)
             * @enum {string}
             */
            type: "function";
            function: components["schemas"]["FunctionDefinition"];
        };
        /** @description Definition of a function that can be called by the model */
        FunctionDefinition: {
            /**
             * @description The name of the function to call
             * @example get_weather
             */
            name: string;
            /**
             * @description A description of what the function does
             * @example Get the current weather in a location
             */
            description?: string;
            /**
             * @description JSON Schema object describing the function parameters
             * @example {
             *       "type": "object",
             *       "properties": {
             *         "location": {
             *           "type": "string",
             *           "description": "The city and state, e.g. San Francisco, CA"
             *         }
             *       },
             *       "required": [
             *         "location"
             *       ]
             *     }
             */
            parameters?: {
                [key: string]: unknown;
            };
            /**
             * @description Whether to enforce strict parameter validation
             * @default false
             */
            strict?: boolean;
        };
        /** @description A tool call made by the model */
        ToolCall: {
            /**
             * @description Unique identifier for this tool call
             * @example call_abc123
             */
            id: string;
            /**
             * @description The type of tool call (currently only "function")
             * @enum {string}
             */
            type: "function";
            function: components["schemas"]["ToolCallFunction"];
        };
        /** @description The function called by the model */
        ToolCallFunction: {
            /**
             * @description The name of the function called
             * @example get_weather
             */
            name: string;
            /**
             * @description JSON string of the arguments to the function
             * @example {"location": "San Francisco, CA"}
             */
            arguments: string;
        };
        /**
         * @description Controls how the model uses tools. Options:
         *     - "auto": Model decides whether to call a tool (default)
         *     - "none": Model will not call any tools
         *     - "required": Model must call at least one tool
         *     - object: Force a specific function to be called
         */
        ToolChoice: ("auto" | "none" | "required") | {
            /** @enum {string} */
            type: "function";
            function: {
                /** @description The name of the function to call */
                name: string;
            };
        };
        /**
         * @description The role of a message sender in a conversation
         * @enum {string}
         */
        Role: "system" | "user" | "assistant" | "tool";
        /**
         * @description Reason why generation stopped
         * @enum {string}
         */
        FinishReason: "stop" | "length" | "tool_calls" | "content_filter" | "function_call";
        /**
         * @description Message content. Supports two formats:
         *     - Simple string: "Hello, how are you?"
         *     - Array of content parts (OpenAI multimodal format): [{"type": "text", "text": "Hello"}]
         */
        ChatMessageContent: string | components["schemas"]["ContentPart"][];
        ChatMessage: {
            role: components["schemas"]["Role"];
            content?: components["schemas"]["ChatMessageContent"];
            /** @description Tool calls made by the assistant (only for role=assistant) */
            tool_calls?: components["schemas"]["ToolCall"][];
            /** @description ID of the tool call this message is responding to (only for role=tool) */
            tool_call_id?: string;
        };
        GenerateRequest: {
            /**
             * @description Name of the generator model from models_dir/generators/
             * @example google/gemma-3-1b-it
             */
            model: string;
            /** @description Conversation messages (OpenAI-compatible format) */
            messages: components["schemas"]["ChatMessage"][];
            /**
             * @description Maximum tokens to generate
             * @default 256
             * @example 256
             */
            max_tokens?: number;
            /**
             * Format: float
             * @description Sampling temperature (0.0 = deterministic, higher = more random)
             * @default 1
             * @example 0.7
             */
            temperature?: number;
            /**
             * Format: float
             * @description Nucleus sampling probability
             * @default 1
             */
            top_p?: number;
            /**
             * @description Top-k sampling (Termite extension, not in OpenAI API)
             * @default 50
             */
            top_k?: number;
            /**
             * @description If true, partial message deltas will be sent as SSE events
             * @default false
             */
            stream?: boolean;
            /**
             * @description List of tools (functions) the model can call.
             *     Only supported by models with tool_call_format configured.
             */
            tools?: components["schemas"]["Tool"][];
            /** @description Controls how the model uses tools */
            tool_choice?: components["schemas"]["ToolChoice"];
        };
        /** @description OpenAI-compatible chat completion response */
        GenerateResponse: {
            /**
             * @description A unique identifier for the chat completion
             * @example chatcmpl-abc123
             */
            id: string;
            /**
             * @description The object type, always "chat.completion"
             * @enum {string}
             */
            object: "chat.completion";
            /**
             * @description Unix timestamp (seconds) when the completion was created
             * @example 1704123456
             */
            created: number;
            /** @description Model used for generation */
            model: string;
            /** @description List of completion choices (currently always 1) */
            choices: components["schemas"]["GenerateChoice"][];
            usage: components["schemas"]["GenerateUsage"];
        };
        GenerateChoice: {
            /** @description Index of this choice in the list */
            index: number;
            message: components["schemas"]["GenerateMessage"];
            finish_reason: components["schemas"]["FinishReason"];
            /** @description Log probability information (not supported, always null) */
            logprobs?: Record<string, never> | null;
        };
        GenerateMessage: {
            role: components["schemas"]["Role"];
            /** @description The generated message content (null when tool_calls is present) */
            content?: string | null;
            /** @description Tool calls made by the model (only present when finish_reason is tool_calls) */
            tool_calls?: components["schemas"]["ToolCall"][];
        };
        GenerateUsage: {
            /** @description Number of tokens in the prompt */
            prompt_tokens: number;
            /** @description Number of tokens in the completion */
            completion_tokens: number;
            /** @description Total tokens used (prompt + completion) */
            total_tokens: number;
        };
        /** @description Streaming generation chunk (SSE event data) */
        GenerateChunk: {
            id: string;
            /** @enum {string} */
            object: "chat.completion.chunk";
            created: number;
            model: string;
            choices: components["schemas"]["GenerateChunkChoice"][];
        };
        GenerateChunkChoice: {
            index: number;
            delta: components["schemas"]["GenerateDelta"];
            finish_reason?: components["schemas"]["FinishReason"];
        };
        /** @description Delta content for streaming */
        GenerateDelta: {
            role?: components["schemas"]["Role"];
            /** @description Token content delta */
            content?: string | null;
            /** @description Tool call deltas for streaming tool calls */
            tool_calls?: components["schemas"]["ToolCallDelta"][];
        };
        /** @description Incremental tool call data for streaming */
        ToolCallDelta: {
            /** @description Index of the tool call in the array */
            index?: number;
            /** @description Unique identifier (only in first delta for this index) */
            id?: string;
            /**
             * @description The type of tool call (only in first delta)
             * @enum {string}
             */
            type?: "function";
            function?: components["schemas"]["ToolCallFunctionDelta"];
        };
        /** @description Incremental function call data for streaming */
        ToolCallFunctionDelta: {
            /** @description Function name (only in first delta) */
            name?: string;
            /** @description Incremental arguments JSON string */
            arguments?: string;
        };
        Config: {
            /**
             * Format: uri
             * @description URL of the Termite embedding/chunking service
             * @example http://localhost:8080
             */
            api_url: string;
            /**
             * @description Base directory containing model subdirectories. Termite auto-discovers models from:
             *     - `{models_dir}/embedders/` - Embedding models (ONNX)
             *     - `{models_dir}/chunkers/` - Chunking models (ONNX)
             *     - `{models_dir}/rerankers/` - Reranking models (ONNX)
             *     - `{models_dir}/recognizers/` - Recognition models (ONNX)
             *     - `{models_dir}/rewriters/` - Seq2Seq rewriter models (ONNX)
             *
             *     Defaults to ~/.termite/models (set via viper). If not set, only built-in fixed chunking is available.
             * @example ~/.termite/models
             */
            models_dir?: string;
            /** @description Security settings for downloading content from URLs (e.g., images for CLIP models). Controls allowed hosts, private IP blocking, download limits, and timeouts. */
            content_security?: components["schemas"]["ContentSecurityConfig"];
            /** @description S3 credentials for downloading content from S3 URLs. If not set, S3 URLs will fail. */
            s3_credentials?: components["schemas"]["Credentials"];
            /**
             * @description How long to keep models loaded in memory after last use (Ollama-compatible).
             *     Models are automatically unloaded after this duration of inactivity.
             *     Use Go duration format: "5m" (5 minutes), "1h" (1 hour), "0" (eager loading).
             *     Defaults to "5m" (lazy loading) like Ollama. Set to "0" to explicitly enable eager loading
             *     where all models are loaded at startup and never unloaded.
             * @default 5m
             * @example 5m
             */
            keep_alive?: string;
            /**
             * @description Maximum number of models to keep loaded in memory simultaneously.
             *     When this limit is reached, the least recently used model is unloaded (LRU eviction).
             *     Set to 0 for unlimited (default). Only effective when keep_alive is non-zero.
             * @default 0
             * @example 3
             */
            max_loaded_models?: number;
            /**
             * @description Number of concurrent inference pipelines per model. Each pipeline loads
             *     a copy of the model, so higher values use more memory but allow more
             *     concurrent requests. Set to 0 to use the default (min(NumCPU, 4)).
             * @default 0
             * @example 1
             */
            pool_size?: number;
            /**
             * @description Backend priority order for model loading with optional device specifiers.
             *     Format: `backend` or `backend:device` where device defaults to `auto`.
             *
             *     Termite tries entries in order and uses the first available backend+device
             *     combination that supports the model.
             *
             *     **Backends** (depend on build tags):
             *     - `go` - Pure Go inference (always available, CPU only, slowest)
             *     - `onnx` - ONNX Runtime (requires -tags="onnx,ORT", fastest)
             *     - `xla` - GoMLX XLA (requires -tags="xla,XLA", TPU/CUDA/CPU)
             *
             *     **Devices**:
             *     - `auto` - Auto-detect best available (default)
             *     - `cuda` - NVIDIA CUDA GPU
             *     - `coreml` - Apple CoreML (macOS only, used by ONNX)
             *     - `tpu` - Google TPU (used by XLA)
             *     - `cpu` - Force CPU only
             *
             *     **Examples**:
             *     - `["onnx", "xla", "go"]` - Try backends with auto device detection
             *     - `["onnx:cuda", "xla:tpu", "onnx:cpu", "go"]` - Prefer GPU, fall back to CPU
             *     - `["onnx:coreml", "go"]` - macOS with CoreML acceleration
             * @default [
             *       "onnx",
             *       "xla",
             *       "go"
             *     ]
             * @example [
             *       "onnx:cuda",
             *       "xla:tpu",
             *       "onnx:cpu",
             *       "xla:cpu",
             *       "go"
             *     ]
             */
            backend_priority?: string[];
            /**
             * @description Maximum number of concurrent inference requests allowed.
             *     Additional requests will be queued up to max_queue_size.
             *     Set to 0 for unlimited (default).
             * @default 0
             * @example 4
             */
            max_concurrent_requests?: number;
            /**
             * @description Maximum number of requests to queue when max_concurrent_requests is reached.
             *     When the queue is full, new requests receive 503 Service Unavailable with Retry-After header.
             *     Set to 0 for unlimited queue (default). Only effective when max_concurrent_requests > 0.
             * @default 0
             * @example 100
             */
            max_queue_size?: number;
            /**
             * @description Maximum time to wait for a request to complete, including queue wait time.
             *     Use Go duration format: "30s", "1m", "0" (no timeout, default).
             *     Requests exceeding this timeout receive 504 Gateway Timeout.
             * @default 0
             * @example 30s
             */
            request_timeout?: string;
            /**
             * @description List of model names to preload at startup (Ollama-compatible).
             *     These models are loaded immediately when Termite starts, avoiding first-request latency.
             *     Model names should match those in models_dir/embedders/ (e.g., "BAAI/bge-small-en-v1.5").
             *     Only effective when keep_alive is non-zero (lazy loading mode).
             * @example [
             *       "BAAI/bge-small-en-v1.5",
             *       "openai/clip-vit-base-patch32"
             *     ]
             */
            preload?: string[];
            /**
             * @description Maximum memory (in MB) to use for loaded models.
             *     When this limit is approached, least recently used models are unloaded.
             *     Set to 0 for unlimited (default). This is an advisory limit - actual memory
             *     usage depends on model sizes and may temporarily exceed this value.
             *     Works alongside max_loaded_models for fine-grained control.
             * @default 0
             * @example 4096
             */
            max_memory_mb?: number;
            /**
             * @description Per-model loading strategy overrides. Maps model names to their loading strategy.
             *     Models not in this map use the default strategy based on keep_alive:
             *     - If keep_alive>0 (default "5m"): lazy loading (load on demand, unload after idle)
             *     - If keep_alive="0": eager loading (load at startup, never unload)
             *
             *     When a model has strategy "eager" in this map:
             *     - It is loaded at startup (as part of preload)
             *     - It is never unloaded, even when keep_alive>0 (pinned in memory)
             *
             *     This allows mixing eager and lazy models in the same pool.
             * @example {
             *       "BAAI/bge-small-en-v1.5": "eager",
             *       "mirth/chonky-mmbert-small-multilingual-1": "lazy"
             *     }
             */
            model_strategies?: {
                [key: string]: "eager" | "lazy" | "bounded";
            };
            log?: components["schemas"]["schemas-Config"];
        };
        VersionResponse: {
            /**
             * @description Termite version
             * @example v1.0.0
             */
            version: string;
            /**
             * @description Git commit hash
             * @example abc1234
             */
            git_commit: string;
            /**
             * @description Build timestamp
             * @example 2024-01-15T10:30:00Z
             */
            build_time: string;
            /**
             * @description Go runtime version
             * @example go1.25.0
             */
            go_version: string;
        };
        ContentSecurityConfig: {
            /**
             * @description Whitelist of allowed hostnames/IPs for link downloads. If empty, all hosts are allowed (except private IPs if block_private_ips is true).
             * @example [
             *       "example.com",
             *       "cdn.example.com",
             *       "192.0.2.1"
             *     ]
             */
            allowed_hosts?: string[];
            /**
             * @description Block requests to private IP ranges (127.0.0.0/8, 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16, 169.254.0.0/16)
             * @default true
             */
            block_private_ips?: boolean;
            /**
             * @description Maximum size of downloaded content in bytes
             * @default 104857600
             * @example 104857600
             */
            max_download_size_bytes?: number;
            /**
             * @description Timeout for individual download operations in seconds
             * @default 30
             * @example 30
             */
            download_timeout_seconds?: number;
            /**
             * @description Maximum image width/height in pixels (images will be resized)
             * @default 2048
             * @example 2048
             */
            max_image_dimension?: number;
            /**
             * @description Whitelist of allowed path prefixes for file:// and s3:// URLs. If empty, all paths are allowed. For file:// use absolute paths (e.g., /Users/data/). For s3:// use bucket/prefix (e.g., my-bucket/uploads/).
             * @example [
             *       "/Users/data/",
             *       "my-bucket/uploads/"
             *     ]
             */
            allowed_paths?: string[];
        };
        Credentials: {
            /**
             * @description S3-compatible endpoint (e.g., 's3.amazonaws.com' or 'localhost:9000' for MinIO)
             * @example s3.amazonaws.com
             */
            endpoint?: string;
            /**
             * @description Enable SSL/TLS for S3 connections (default: true for AWS, false for local MinIO)
             * @default true
             */
            use_ssl?: boolean;
            /**
             * @description AWS access key ID. Supports keystore syntax for secret lookup. Falls back to AWS_ACCESS_KEY_ID environment variable if not set.
             * @example your-access-key-id
             */
            access_key_id?: string;
            /**
             * @description AWS secret access key. Supports keystore syntax for secret lookup. Falls back to AWS_SECRET_ACCESS_KEY environment variable if not set.
             * @example your-secret-access-key
             */
            secret_access_key?: string;
            /**
             * @description Optional AWS session token for temporary credentials. Supports keystore syntax for secret lookup.
             * @example your-session-token
             */
            session_token?: string;
        };
        /**
         * @description Logging verbosity level
         * @default info
         * @example info
         * @enum {string}
         */
        Level: "debug" | "info" | "warn" | "error";
        /**
         * @description Logging output format style. 'terminal' for colorized console, 'json' for structured JSON, 'logfmt' for token-efficient key=value pairs, 'noop' for silent.
         * @default terminal
         * @example terminal
         * @enum {string}
         */
        Style: "terminal" | "json" | "logfmt" | "noop";
        /** @description Logging configuration for Termite services */
        "schemas-Config": {
            level?: components["schemas"]["Level"];
            style?: components["schemas"]["Style"];
        };
    };
    responses: never;
    parameters: never;
    requestBodies: never;
    headers: never;
    pathItems: never;
}
export type $defs = Record<string, never>;
export interface operations {
    generateEmbeddings: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["EmbedRequest"];
            };
        };
        responses: {
            /** @description Embeddings generated successfully */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/octet-stream": string;
                    "application/json": components["schemas"]["EmbedResponse"];
                };
            };
            /** @description Invalid request */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Model not found */
            404: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Internal server error */
            500: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
        };
    };
    chunkText: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["ChunkRequest"];
            };
        };
        responses: {
            /** @description Text chunked successfully */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["ChunkResponse"];
                };
            };
            /** @description Invalid request */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Internal server error */
            500: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
        };
    };
    rerankPrompts: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["RerankRequest"];
            };
        };
        responses: {
            /** @description Prompts reranked successfully */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["RerankResponse"];
                };
            };
            /** @description Invalid request */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Model not found */
            404: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Internal server error */
            500: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Reranking service unavailable (no models configured) */
            503: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
        };
    };
    generateContent: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["GenerateRequest"];
            };
        };
        responses: {
            /**
             * @description Chat completion response. Returns JSON for non-streaming requests,
             *     or Server-Sent Events for streaming requests (stream: true).
             */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["GenerateResponse"];
                    "text/event-stream": components["schemas"]["GenerateChunk"];
                };
            };
            /** @description Invalid request */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Model not found */
            404: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Internal server error */
            500: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Generation service unavailable (no models configured) */
            503: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
        };
    };
    recognizeEntities: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["RecognizeRequest"];
            };
        };
        responses: {
            /** @description Entities extracted successfully */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["RecognizeResponse"];
                };
            };
            /** @description Invalid request */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Model not found */
            404: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Internal server error */
            500: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Recognition service unavailable (no models configured) */
            503: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
        };
    };
    classifyText: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["ClassifyRequest"];
            };
        };
        responses: {
            /** @description Classification completed successfully */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["ClassifyResponse"];
                };
            };
            /** @description Invalid request */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Model not found */
            404: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Internal server error */
            500: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Classification service unavailable (no models configured) */
            503: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
        };
    };
    rewriteText: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["RewriteRequest"];
            };
        };
        responses: {
            /** @description Text rewritten successfully */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["RewriteResponse"];
                };
            };
            /** @description Invalid request */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Model not found */
            404: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Internal server error */
            500: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Generation service unavailable (no models configured) */
            503: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
        };
    };
    readImages: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["ReadRequest"];
            };
        };
        responses: {
            /** @description Images read successfully */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["ReadResponse"];
                };
            };
            /** @description Invalid request */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Model not found */
            404: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Internal server error */
            500: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Reader service unavailable (no models configured) */
            503: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
        };
    };
    transcribeAudio: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["TranscribeRequest"];
            };
        };
        responses: {
            /** @description Audio transcribed successfully */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["TranscribeResponse"];
                };
            };
            /** @description Invalid request */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Model not found */
            404: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Internal server error */
            500: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Transcription service unavailable (no models configured) */
            503: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
        };
    };
    listModels: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Models retrieved successfully */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["ModelsResponse"];
                };
            };
            /** @description Bad request */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
            /** @description Internal server error */
            500: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
        };
    };
    getVersion: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Version information */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["VersionResponse"];
                };
            };
            /** @description Bad request */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Error"];
                };
            };
        };
    };
}
